{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c95ef90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f374c359d60>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "# import gradio as gr\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "from einops import repeat, rearrange\n",
    "from pytorch_lightning import seed_everything\n",
    "import os\n",
    "from imwatermark import WatermarkEncoder\n",
    "import imgviz\n",
    "sys.path.append(\"./stablediffusion\")\n",
    "from stablediffusion.scripts.txt2img import put_watermark\n",
    "from stablediffusion.ldm.util import instantiate_from_config\n",
    "from stablediffusion.ldm.models.diffusion.ddim import DDIMSampler\n",
    "from stablediffusion.ldm.data.util import AddMiDaS\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5138fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_sd(\n",
    "        image,\n",
    "        txt,\n",
    "        device,\n",
    "        num_samples=1,\n",
    "        model_type=\"dpt_hybrid\"\n",
    "):\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n",
    "    # sample['jpg'] is tensor hwc in [-1, 1] at this point\n",
    "    midas_trafo = AddMiDaS(model_type=model_type)\n",
    "    batch = {\n",
    "        \"jpg\": image,\n",
    "        \"txt\": num_samples * [txt],\n",
    "    }\n",
    "    batch = midas_trafo(batch)\n",
    "    batch[\"jpg\"] = rearrange(batch[\"jpg\"], 'h w c -> 1 c h w')\n",
    "    batch[\"jpg\"] = repeat(batch[\"jpg\"].to(device=device),\n",
    "                          \"1 ... -> n ...\", n=num_samples)\n",
    "    batch[\"midas_in\"] = repeat(torch.from_numpy(batch[\"midas_in\"][None, ...]).to(\n",
    "        device=device), \"1 ... -> n ...\", n=num_samples)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def paint(sampler, image, prompt, t_enc, seed, scale, num_samples=1, callback=None,\n",
    "          do_full_sample=False):\n",
    "    device = torch.device(\n",
    "        \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = sampler.model\n",
    "    seed_everything(seed)\n",
    "\n",
    "    print(\"Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\")\n",
    "    wm = \"SDV2\"\n",
    "    wm_encoder = WatermarkEncoder()\n",
    "    wm_encoder.set_watermark('bytes', wm.encode('utf-8'))\n",
    "\n",
    "    with torch.no_grad(),\\\n",
    "            torch.autocast(\"cuda\"):\n",
    "        batch = make_batch_sd(\n",
    "            image, txt=prompt, device=device, num_samples=num_samples)\n",
    "        z = model.get_first_stage_encoding(model.encode_first_stage(\n",
    "            batch[model.first_stage_key]))  # move to latent space\n",
    "        c = model.cond_stage_model.encode(batch[\"txt\"])\n",
    "        c_cat = list()\n",
    "        for ck in model.concat_keys:\n",
    "            cc = batch[ck]\n",
    "            cc = model.depth_model(cc)\n",
    "            depth_min, depth_max = torch.amin(cc, dim=[1, 2, 3], keepdim=True), torch.amax(cc, dim=[1, 2, 3],\n",
    "                                                                                           keepdim=True)\n",
    "            display_depth = (cc - depth_min) / (depth_max - depth_min)\n",
    "            depth_image = Image.fromarray(\n",
    "                (display_depth[0, 0, ...].cpu().numpy() * 255.).astype(np.uint8))\n",
    "            cc = torch.nn.functional.interpolate(\n",
    "                cc,\n",
    "                size=z.shape[2:],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            depth_min, depth_max = torch.amin(cc, dim=[1, 2, 3], keepdim=True), torch.amax(cc, dim=[1, 2, 3],\n",
    "                                                                                           keepdim=True)\n",
    "            cc = 2. * (cc - depth_min) / (depth_max - depth_min) - 1.\n",
    "            c_cat.append(cc)\n",
    "        c_cat = torch.cat(c_cat, dim=1)\n",
    "        # cond\n",
    "        cond = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n",
    "\n",
    "        # uncond cond\n",
    "        uc_cross = model.get_unconditional_conditioning(num_samples, \"\")\n",
    "        uc_full = {\"c_concat\": [c_cat], \"c_crossattn\": [uc_cross]}\n",
    "        if not do_full_sample:\n",
    "            # encode (scaled latent)\n",
    "            z_enc = sampler.stochastic_encode(\n",
    "                z, torch.tensor([t_enc] * num_samples).to(model.device))\n",
    "        else:\n",
    "            z_enc = torch.randn_like(z)\n",
    "        # decode it\n",
    "        samples = sampler.decode(z_enc, cond, t_enc, unconditional_guidance_scale=scale,\n",
    "                                 unconditional_conditioning=uc_full, callback=callback)\n",
    "        x_samples_ddim = model.decode_first_stage(samples)\n",
    "        result = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "        result = result.cpu().numpy().transpose(0, 2, 3, 1) * 255\n",
    "    return [depth_image] + [put_watermark(Image.fromarray(img.astype(np.uint8)), wm_encoder) for img in result]\n",
    "\n",
    "\n",
    "def pad_image(input_image):\n",
    "    pad_w, pad_h = np.max(((2, 2), np.ceil(\n",
    "        np.array(input_image.size) / 64).astype(int)), axis=0) * 64 - input_image.size\n",
    "    im_padded = Image.fromarray(\n",
    "        np.pad(np.array(input_image), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n",
    "    w, h = im_padded.size\n",
    "    if w == h:\n",
    "        return im_padded\n",
    "    elif w > h:\n",
    "        new_image = Image.new(im_padded.mode, (w, w), (0, 0, 0))\n",
    "        new_image.paste(im_padded, (0, (w - h) // 2))\n",
    "        return new_image\n",
    "    else:\n",
    "        new_image = Image.new(im_padded.mode, (h, h), (0, 0, 0))\n",
    "        new_image.paste(im_padded, ((h - w) // 2, 0))\n",
    "        return new_image\n",
    "\n",
    "\n",
    "def predict(input_image, prompt, steps, num_samples, scale, seed, eta, strength):\n",
    "    num_samples = 1\n",
    "    init_image = input_image.convert(\"RGB\")\n",
    "    image = pad_image(init_image)  # resize to integer multiple of 32\n",
    "    image = image.resize((512, 512))\n",
    "    sampler.make_schedule(steps, ddim_eta=eta, verbose=True)\n",
    "    assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "    do_full_sample = strength == 1.\n",
    "    t_enc = min(int(strength * steps), steps-1)\n",
    "    result = paint(\n",
    "        sampler=sampler,\n",
    "        image=image,\n",
    "        prompt=prompt,\n",
    "        t_enc=t_enc,\n",
    "        seed=seed,\n",
    "        scale=scale,\n",
    "        num_samples=num_samples,\n",
    "        callback=None,\n",
    "        do_full_sample=do_full_sample\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def initialize_model(config, ckpt):\n",
    "    config = OmegaConf.load(config)\n",
    "    model = instantiate_from_config(config.model)\n",
    "    model.load_state_dict(torch.load(ckpt)[\"state_dict\"], strict=False)\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    sampler = DDIMSampler(model)\n",
    "    return sampler\n",
    "\n",
    "os.makedirs(\".out\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cdb824f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n",
      "LatentDepth2ImageDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "fpath = \"stablediffusion/configs/stable-diffusion/v2-midas-inference.yaml\"\n",
    "wpath = \"weights/512-depth-ema.ckpt\"\n",
    "sampler = initialize_model(fpath, wpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9da865df",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'comm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m list_of_frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msort([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(video_path) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m i])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fid, fname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(list_of_frames):\n\u001b[0;32m---> 13\u001b[0m     input_image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     result \u001b[38;5;241m=\u001b[39m predict(input_image, prompt, ddim_steps, num_samples, scale, seed, eta, strength)\n\u001b[1;32m     15\u001b[0m     output_image \u001b[38;5;241m=\u001b[39m imgviz\u001b[38;5;241m.\u001b[39mtile([np\u001b[38;5;241m.\u001b[39marray(input_image\u001b[38;5;241m.\u001b[39mresize(result[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msize)), np\u001b[38;5;241m.\u001b[39marray(result[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mresize(result[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msize)), np\u001b[38;5;241m.\u001b[39marray(result[\u001b[38;5;241m1\u001b[39m])], shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), border\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/StableVideos/lib/python3.9/site-packages/ipywidgets/widgets/widget.py:517\u001b[0m, in \u001b[0;36mWidget.open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[39m\"\"\"Open a comm to the frontend if one isn't already open.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcomm \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m         state, buffer_paths, buffers \u001b[39m=\u001b[39m _remove_buffers(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_state())\n\u001b[1;32m    520\u001b[0m         args \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(target_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mjupyter.widget\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    521\u001b[0m                     data\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m: state, \u001b[39m'\u001b[39m\u001b[39mbuffer_paths\u001b[39m\u001b[39m'\u001b[39m: buffer_paths},\n\u001b[1;32m    522\u001b[0m                     buffers\u001b[39m=\u001b[39mbuffers,\n\u001b[1;32m    523\u001b[0m                     metadata\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m'\u001b[39m: __protocol_version__}\n\u001b[1;32m    524\u001b[0m                     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'comm'"
     ]
    }
   ],
   "source": [
    "prompt = \"A high resolution photo of a woman with a red dress and a black hat.\"\n",
    "ddim_steps = 50\n",
    "num_samples = 1\n",
    "scale = 5.0\n",
    "seed = 42\n",
    "eta = 0.0\n",
    "strength = 0.9\n",
    "\n",
    "video_path = \"assets/videos/youtube_run_000/img/\"\n",
    "save_path = \"out/v0_test1.mp4\"\n",
    "list_of_frames = np.sort([i for i in os.listdir(video_path) if \".jpg\" in i])\n",
    "for fid, fname in enumerate(list_of_frames):\n",
    "    input_image = Image.open(video_path + fname)\n",
    "    result = predict(input_image, prompt, ddim_steps, num_samples, scale, seed, eta, strength)\n",
    "    output_image = imgviz.tile([np.array(input_image.resize(result[1].size)), np.array(result[0].resize(result[1].size)), np.array(result[1])], shape=(1, 3), border=(255, 255, 255))\n",
    "    output_image = output_image[:, :, ::-1]\n",
    "    if(fid==0):\n",
    "        video_file = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, frameSize=(output_image.shape[1],output_image.shape[0]))\n",
    "    video_file.write(output_image)\n",
    "video_file.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11868144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"80%\" controls>\n",
       "  <source src=\"out/v0_test1.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"80%\" controls>\n",
    "  <source src=\"out/v0_test1.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15897b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a267f3457c1544b3881e7b540850231d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00\\x1cftypisom\\x00\\x00\\x02\\x00isomiso2mp41\\x00\\x00\\x00\\x08free\\x00:I\\xbf...', height='â€¦"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import Video, Image\n",
    "video = Video.from_file(\"out/v0_test1.mp4\",play=True,width=1400, height=1400)\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24514f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('StableVideos')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d89d0ed29e57dd481223cf631b2a285073732841785c2ef7d547de13c0f3d1af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
